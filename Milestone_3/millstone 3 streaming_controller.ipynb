{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33063d13-a0a0-4d75-91bb-8974cc330dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    from_json, col, to_timestamp, window,\n",
    "    sum as _sum, avg as _avg, when, to_json, struct\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "\n",
    "cfg_path = os.path.join(\"config_m3.yaml\")\n",
    "with open(cfg_path) as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "kafka_boot = cfg['kafka']['bootstrap_servers']\n",
    "input_topic = cfg['kafka']['input_topic']\n",
    "alert_topic = cfg['kafka']['alert_topic']\n",
    "\n",
    "w_dur = cfg['streaming']['window_duration']\n",
    "s_dur = cfg['streaming']['slide_duration']\n",
    "thr_med = cfg['streaming']['vehicle_threshold_moderate']\n",
    "thr_high = cfg['streaming']['vehicle_threshold_high']\n",
    "\n",
    "checkpoint = cfg['streaming']['checkpoint_dir']\n",
    "parquet_out = cfg['paths']['parquet_output']\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RealTimeTrafficAnalytics\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "\n",
    "raw = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_boot)\n",
    "    .option(\"subscribe\", input_topic)\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"street_name\", StringType()),\n",
    "    StructField(\"vehicle_count\", IntegerType()),\n",
    "    StructField(\"avg_speed\", DoubleType())\n",
    "])\n",
    "\n",
    "json_str = raw.selectExpr(\"CAST(value AS STRING) as json_str\")\n",
    "parsed = json_str.select(from_json(col(\"json_str\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "parsed = parsed.withColumn(\"event_time\", to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "\n",
    "parsed_with_watermark = parsed.withWatermark(\"event_time\", \"30 seconds\") \n",
    "\n",
    "agg = (\n",
    "    parsed_with_watermark # استخدام DataFrame مع العلامة المائية\n",
    "    .groupBy(window(col(\"event_time\"), w_dur, s_dur), col(\"street_name\"))\n",
    "    .agg(\n",
    "        _sum(\"vehicle_count\").alias(\"vehicles_per_window\"),\n",
    "        _avg(\"avg_speed\").alias(\"avg_speed\")\n",
    "    )\n",
    "    .select(\n",
    "        col(\"window\").start.alias(\"window_start\"),\n",
    "        col(\"window\").end.alias(\"window_end\"),\n",
    "        col(\"street_name\"),\n",
    "        col(\"vehicles_per_window\"),\n",
    "        col(\"avg_speed\")\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "agg2 = (\n",
    "    agg\n",
    "    .withColumn(\n",
    "        \"alert_level\",\n",
    "        when(col(\"avg_speed\") < 15, \"HIGH\")           \n",
    "        .when(col(\"avg_speed\") < 25, \"MEDIUM\")         \n",
    "        .when(col(\"vehicles_per_window\") > thr_high, \"HIGH\")  \n",
    "        .when(col(\"vehicles_per_window\") > thr_med, \"MEDIUM\") \n",
    "        .otherwise(\"LOW\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"is_alert\",\n",
    "        (\n",
    "            (col(\"avg_speed\") < 25) | (col(\"vehicles_per_window\") > thr_med)\n",
    "        ).cast(\"boolean\")\n",
    "    )\n",
    ")\n",
    "\n",
    "agg2_clean = agg2.fillna(0.0, subset=['vehicles_per_window', 'avg_speed'])\n",
    "\n",
    "\n",
    "alerts_for_kafka = (\n",
    "    agg2_clean \n",
    "    .filter(col(\"is_alert\") == True)\n",
    "    .select(to_json(struct(*[col(c) for c in agg2_clean.columns])).alias(\"value\"))\n",
    ")\n",
    "\n",
    "alerts_query = (\n",
    "    alerts_for_kafka\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_boot)\n",
    "    .option(\"topic\", alert_topic)\n",
    "    .option(\"checkpointLocation\", checkpoint + \"/alerts_kafka\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "\n",
    "parquet_query = (\n",
    "    agg2_clean \n",
    "    .writeStream\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", parquet_out)\n",
    "    .option(\"checkpointLocation\", checkpoint + \"/parquet\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "\n",
    "console_query = (\n",
    "    agg2_clean \n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"update\")\n",
    "    .option(\"truncate\", False)\n",
    "    .start()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec25a72-a609-4714-b9db-235abc2bb231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import paho.mqtt.client as mqtt\n",
    "import pandas as pd\n",
    "from datetime import datetime \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    from_json, col, to_timestamp, window, sum as _sum, avg as _avg,\n",
    "    when, to_json, struct\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "\n",
    "cfg_path = os.path.join(\"config_m3.yaml\")\n",
    "with open(cfg_path) as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "KAFKA_BOOT = cfg['kafka']['bootstrap_servers']\n",
    "INPUT_TOPIC = cfg['kafka']['input_topic']\n",
    "ALERT_TOPIC = cfg['kafka']['alert_topic']\n",
    "\n",
    "WINDOW_DURATION = cfg['streaming']['window_duration']\n",
    "SLIDE_DURATION = cfg['streaming']['slide_duration']\n",
    "VEHICLE_THRESHOLD_MED = cfg['streaming']['vehicle_threshold_moderate']\n",
    "VEHICLE_THRESHOLD_HIGH = cfg['streaming']['vehicle_threshold_high']\n",
    "\n",
    "CHECKPOINT_DIR = cfg['streaming']['checkpoint_dir']\n",
    "LIGHT_TOPIC_PREFIX = cfg['lighting_control']['lighting_topic_prefix']\n",
    "\n",
    "MQTT_HOST = cfg['mqtt']['host']\n",
    "MQTT_PORT = cfg['mqtt']['port']\n",
    "\n",
    "-\n",
    "spark = SparkSession.builder.appName(\"LightingController\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "\n",
    "print(\"Stopping any previously active streaming queries...\")\n",
    "for q in spark.streams.active:\n",
    "    try:\n",
    "        q.stop()\n",
    "        print(f\"Query '{q.name}' stopped.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error stopping query '{q.name}': {e}\")\n",
    "print(\"--- All previous queries stopped. ---\")\n",
    "\n",
    "\n",
    "raw = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOT)\n",
    "    .option(\"subscribe\", INPUT_TOPIC)\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"street_name\", StringType()),\n",
    "    StructField(\"vehicle_count\", IntegerType()),\n",
    "    StructField(\"avg_speed\", DoubleType())\n",
    "])\n",
    "\n",
    "json_str = raw.selectExpr(\"CAST(value AS STRING) as json_str\")\n",
    "parsed = json_str.select(from_json(col(\"json_str\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "parsed = parsed.withColumn(\"event_time\", to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "\n",
    "agg_input = parsed.withWatermark(\"event_time\", \"1 minute\")\n",
    "\n",
    "agg = (\n",
    "    agg_input.groupBy(window(col(\"event_time\"), WINDOW_DURATION, SLIDE_DURATION), col(\"street_name\"))\n",
    "    .agg(\n",
    "        _sum(\"vehicle_count\").alias(\"vehicles_per_window\"),\n",
    "        _avg(\"avg_speed\").alias(\"avg_speed\")\n",
    "    )\n",
    "    .select(\n",
    "        col(\"window\").start.alias(\"window_start\"),\n",
    "        col(\"window\").end.alias(\"window_end\"),\n",
    "        col(\"street_name\"),\n",
    "        col(\"vehicles_per_window\"),\n",
    "        col(\"avg_speed\")\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "agg2 = (\n",
    "    agg.withColumn(\n",
    "        \"alert_level\",\n",
    "        when(col(\"avg_speed\") < 15, \"HIGH\")\n",
    "        .when(col(\"avg_speed\") < 25, \"MEDIUM\")\n",
    "        .when(col(\"vehicles_per_window\") > VEHICLE_THRESHOLD_HIGH, \"HIGH\")\n",
    "        .when(col(\"vehicles_per_window\") > VEHICLE_THRESHOLD_MED, \"MEDIUM\")\n",
    "        .otherwise(\"LOW\")\n",
    "    ).withColumn(\n",
    "        \"is_alert\",\n",
    "        ((col(\"avg_speed\") < 25) | (col(\"vehicles_per_window\") > VEHICLE_THRESHOLD_MED)).cast(\"boolean\")\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "mqtt_client = mqtt.Client(mqtt.CallbackAPIVersion.VERSION2)\n",
    "mqtt_client.connect(MQTT_HOST, MQTT_PORT, 60)\n",
    "\n",
    "def publish_to_mqtt(df, epoch_id):\n",
    "    \n",
    "    pdf = df.toPandas()\n",
    "    for _, row in pdf.iterrows():\n",
    "        street = row['street_name']\n",
    "        level = row['alert_level']\n",
    "        topic = f\"{LIGHT_TOPIC_PREFIX}/{street}/set\"\n",
    "        \n",
    "     \n",
    "        payload = {\n",
    "            \"street\": street, \n",
    "            \"level\": level,\n",
    "            \"avg_speed\": row['avg_speed'] if not pd.isna(row['avg_speed']) else 0.0,\n",
    "            \"vehicles\": row['vehicles_per_window'] if not pd.isna(row['vehicles_per_window']) else 0\n",
    "        }\n",
    "        \n",
    "        mqtt_client.publish(topic, json.dumps(payload))\n",
    "        print(f\"Published to {topic}: {payload}\")\n",
    "\n",
    "\n",
    "current_timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "agg2.writeStream.foreachBatch(publish_to_mqtt)\\\n",
    "    .option(\"checkpointLocation\", os.path.join(CHECKPOINT_DIR, f\"lighting_controller_{current_timestamp}\"))\\\n",
    "    .outputMode(\"update\")\\\n",
    "    .queryName(\"mqtt_lighting_controller\")\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
